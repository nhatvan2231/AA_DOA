{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e559a1d-8446-4ca7-84d8-efc7db330869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 19:54:43.705618: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744055683.718899 3770876 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744055683.723236 3770876 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-07 19:54:43.737620: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import random\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5139c6c6-6cfd-4435-a035-092d534f17fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mMiniPINN\u001b[39;00m(\u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_channels, array_geometry, he_initializer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, floormod\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "class MiniPINN(tf.keras.Model):\n",
    "    def __init__(self, n_channels, array_geometry, he_initializer = True, floormod=False):\n",
    "        super().__init__()\n",
    "        # Fixed metrics\n",
    "        self.mse_metric = CustomMSE(name='mse')\n",
    "        self.mae_metric = CustomMAE(name='mae')\n",
    "\n",
    "        # Use floormod\n",
    "        self.floormod = floormod\n",
    "       \n",
    "        # Target array geometry\n",
    "        self.array_geometry = array_geometry\n",
    "        \n",
    "        \n",
    "        self.initializer = tf.keras.initializers.HeNormal(1561)\n",
    "        if not he_initializer:\n",
    "            self.initializer = tf.keras.initializers.GlorotUniform(1561)\n",
    "        #self.net = tf.keras.Sequential([\n",
    "        #    tf.keras.layers.Input(n_channels),\n",
    "        #    #tf.keras.layers.Dense(units=512, activation='relu', kernel_initializer = self.initializer),\n",
    "        #    #tf.keras.layers.Dense(units=256, activation='relu', kernel_initializer = self.initializer),\n",
    "        #    tf.keras.layers.Dense(units=128, activation='relu', kernel_initializer = self.initializer),\n",
    "        #    tf.keras.layers.Dense(units=64, activation='relu', kernel_initializer = self.initializer),\n",
    "        #    tf.keras.layers.Dense(units=32, activation='relu', kernel_initializer = self.initializer),\n",
    "        #    tf.keras.layers.Dense(units=16, activation='relu', kernel_initializer = self.initializer),\n",
    "        #    #tf.keras.layers.Dense(units=8, activation='relu', kernel_initializer = self.initializer),\n",
    "        #]) \n",
    "        self.theta = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(units=512, activation='relu', kernel_initializer = self.initializer),\n",
    "            tf.keras.layers.Dense(units=256, activation='relu', kernel_initializer = self.initializer),\n",
    "            tf.keras.layers.Dense(units=128, activation='relu', kernel_initializer = self.initializer),\n",
    "            tf.keras.layers.Dense(units=64, activation='relu', kernel_initializer = self.initializer),\n",
    "            tf.keras.layers.Dense(units=32, activation='relu', kernel_initializer = self.initializer),\n",
    "            tf.keras.layers.Dense(units=16, activation='relu', kernel_initializer = self.initializer),\n",
    "            tf.keras.layers.Dense(units=8, activation='relu', kernel_initializer = self.initializer),\n",
    "            tf.keras.layers.Dense(units=1, activation = 'linear')\n",
    "        ]) \n",
    "        #self.c = tf.keras.Sequential([\n",
    "        #    tf.keras.layers.Dense(units=128, activation='relu', kernel_initializer = self.initializer),\n",
    "        #    tf.keras.layers.Dense(units=64, activation='relu', kernel_initializer = self.initializer),\n",
    "        #    tf.keras.layers.Dense(units=32, activation='relu', kernel_initializer = self.initializer),\n",
    "        #    tf.keras.layers.Dense(units=16, activation='relu', kernel_initializer = self.initializer),\n",
    "        #    #tf.keras.layers.Dense(units=16, activation='relu', kernel_initializer = self.initializer),\n",
    "        #    tf.keras.layers.Dense(units=8, activation='relu', kernel_initializer = self.initializer),\n",
    "        #    tf.keras.layers.Dense(units=1, activation = 'linear')\n",
    "        #]) \n",
    "        #self.theta = tf.keras.layers.Dense(units=1, activation = 'linear')\n",
    "        #self.c = tf.keras.layers.Dense(units=1, activation = 'linear')\n",
    "\n",
    "        #self.lambda_theta = tf.keras.Variable(1.0, name='lambda_theta', dtype=tf.float32)  \n",
    "        #self.lambda_c = tf.keras.Variable(1.0, name='lambda_c', dtype=tf.float32)  \n",
    "\n",
    "    def compile(self, *args, **kwargs):\n",
    "        super().compile(*args, **kwargs)\n",
    "\n",
    "    def compute_loss(self, y, y_pred):\n",
    "        # unpacking\n",
    "        #theta_actual, c_actual = tf.unstack(y, axis=-1)\n",
    "        #taus_pred, c_pred = y_pred[0], y_pred[1]\n",
    "        #theta_pred, c_pred = tf.unstack(y_pred, axis=-1)\n",
    "        # MSE loss\n",
    "        #cossin_loss = self.lambda_theta*self.loss.cossin_loss(theta_actual, theta_pred)\n",
    "        cossin_loss = self.loss.cossin_loss(y, y_pred)\n",
    "        # PINN loss\n",
    "        #pinn_loss = self.clambda[1]*self.pinn_loss(taus_actual, taus_pred, c_pred)\n",
    "        # speed los\n",
    "        #c_loss = self.lambda_c*self.loss.speed_loss(c_actual, c_pred)\n",
    "#\n",
    "        #tf.print(c_loss, cossin_loss)\n",
    "#\n",
    "        # Total loss\n",
    "        #loss = cossin_loss + pinn_loss + c_loss\n",
    "        #return cossin_loss + c_loss\n",
    "        return cossin_loss\n",
    "    def train_step(self, dataset):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        taus, c, theta = dataset\n",
    "        with tf.GradientTape() as tape:\n",
    "            #theta_pred, c_pred = self(taus, training=True)  # Forward pass\n",
    "            theta_pred = self(taus, training=True)  # Forward pass\n",
    "            #c_pred = tf.expand_dims(c_pred, axis=-1)\n",
    "            #theta_pred = tf.expand_dims(theta_pred, axis=-1)\n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            # packing the theta and speed\n",
    "            #y = tf.concat([theta,c], axis=-1)\n",
    "            #y_pred = tf.concat([theta_pred, c_pred], axis=-1)\n",
    "            #loss = self.compute_loss(y=y, y_pred=y_pred)\n",
    "            loss = self.compute_loss(y=theta, y_pred=theta_pred)\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        #self.compiled_metrics.update_state(y, y_pred)\n",
    "        \n",
    "        # Update metrics\n",
    "        for metric in self.metrics:\n",
    "            if metric.name == \"loss\":\n",
    "                metric.update_state(loss)\n",
    "            else:\n",
    "                metric.update_state(theta, theta_pred)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "        \n",
    "    def test_step(self, dataset):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        taus, c, theta = dataset\n",
    "        theta_pred = self(taus, training=True)  # Forward pass\n",
    "        #theta_pred, c_pred = self(taus, training=False)  # Forward pass\n",
    "        #c_pred = tf.expand_dims(c_pred, axis=-1)\n",
    "        #theta_pred = tf.expand_dims(theta_pred, axis=-1)\n",
    "        ## Compute the loss value\n",
    "        ## (the loss function is configured in `compile()`)\n",
    "        ## packing the taus and speed\n",
    "        #y = tf.concat([theta,c], axis=-1)\n",
    "        #y_pred = tf.concat([theta_pred, c_pred], axis=-1)\n",
    "        #loss = self.compute_loss(y=y, y_pred=y_pred)\n",
    "        loss = self.compute_loss(y=theta, y_pred=theta_pred)\n",
    "        # Compute gradients\n",
    "        # Update metrics\n",
    "        for metric in self.metrics:\n",
    "            if metric.name == \"loss\":\n",
    "                metric.update_state(loss)\n",
    "            else:\n",
    "                metric.update_state(theta, theta_pred)\n",
    "        \n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def predict(self, inputs,verbose=False):\n",
    "        #output = self.net(inputs)\n",
    "        theta = self.theta(inputs)\n",
    "        #c = self.c(inputs)\n",
    "        #theta, c = output[0], output[1]\n",
    "        if self.floormod:\n",
    "            theta = tf.math.floormod(theta,360.0)\n",
    "        #return theta, c\n",
    "        return theta\n",
    "    def call(self, inputs):\n",
    "        #output = self.net(inputs)\n",
    "        theta = self.theta(inputs)\n",
    "        #c = self.c(inputs)\n",
    "        #theta, c = output[0], output[1]\n",
    "        return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed4cf220-a56d-47dc-ad45-f5ab0a4910a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mMiniPINNLoss\u001b[39;00m(\u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mLoss):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# MniPINN Loss introducing physic to the model\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Taus = R * cos(Theta) / C\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Taus is time delays\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# R reference for a fixed reference time delay\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# R is the array geometry\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# C is speed\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, target_aa_geo, ref_aa_geo, norm, clambda\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1.0\u001b[39m,\u001b[38;5;241m1.0\u001b[39m]):\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "class MiniPINNLoss(tf.keras.losses.Loss):\n",
    "    # MniPINN Loss introducing physic to the model\n",
    "    # Taus = R * cos(Theta) / C\n",
    "    # Taus is time delays\n",
    "    # R reference for a fixed reference time delay\n",
    "    # R is the array geometry\n",
    "    # C is speed\n",
    "    def __init__(self):#, target_aa_geo=None, ref_aa_geo=None, norm=None):#, clambda=[1.0, 1.0, 1.0]):\n",
    "        super().__init__()\n",
    "        #self.r0 = ref_aa_geo\n",
    "        #self.r = target_aa_geo # (num of channels,2)\n",
    "        #self.norm = norm\n",
    "        #self.taus = None\n",
    "    def set_param(self, taus):\n",
    "        self.taus = taus\n",
    "    def cossin_loss(self, y_actual, y_pred):\n",
    "        # convert degree to radian\n",
    "        actual_rad = y_actual*np.pi/180\n",
    "        pred_rad = y_pred*np.pi/180\n",
    "    \n",
    "        # the cosine and sine loss\n",
    "        cos_loss = tf.math.reduce_mean(tf.math.square(1 - tf.math.cos(actual_rad - pred_rad)))\n",
    "        sin_loss = tf.math.reduce_mean(tf.math.square(tf.math.sin(actual_rad - pred_rad)))\n",
    "        \n",
    "        # loss \n",
    "        loss = (cos_loss + sin_loss)\n",
    "        return loss\n",
    "\n",
    "    def speed_loss(self, c_actual, c_pred):\n",
    "        loss = tf.math.reduce_mean(tf.math.square(c_actual - c_pred))\n",
    "        return loss\n",
    "        \n",
    "    #def pinn_loss(self, y_actual, y_pred, c_pred):\n",
    "    #    # convert degree to radian\n",
    "    #    actual_rad = y_actual*np.pi/180\n",
    "    #    pred_rad = y_pred*np.pi/180\n",
    "#\n",
    "    #    # slowness vector (2,None)\n",
    "    #    # since only dealing with azimuth\n",
    "    #    x_dir = tf.math.cos(pred_rad) / c_pred\n",
    "    #    y_dir = tf.math.sin(pred_rad) / c_pred\n",
    "    #    slowness = tf.stack([x_dir, y_dir],1) #/ c_pred\n",
    "#\n",
    "    #    # calculate the time delay for the reference channel\n",
    "    #    taus_ref = -tf.tensordot(slowness, self.r0, axes=[[1],[0]]) / self.norm\n",
    "    #    taus_ref = tf.reshape(taus_ref, [-1,1,1])\n",
    "#\n",
    "    #    # calculate the time delays from predicted angle\n",
    "    #    taus_hat = -tf.tensordot(slowness, self.r, axes = [[1],[1]]) / self.norm\n",
    "    #    # shift the predicted time delays to the reference channel\n",
    "    #    taus_hat -= taus_ref\n",
    "    #    # match the predicted taus dimension\n",
    "    #    taus = tf.expand_dims(self.taus, 1)\n",
    "    #    \n",
    "    #    # calculate the physical loss\n",
    "    #    loss = tf.math.reduce_mean(tf.math.square(taus - taus_hat))\n",
    "    #    return loss\n",
    "        \n",
    "    def call(self, y_actual, y_pred):\n",
    "        raise NotImplementedError(\"Loss is used only as helper, model handles total loss.\")\n",
    "        ## unpacking\n",
    "        #taus_actual, c_actual = y_actual[0], y_actual[1]\n",
    "        #taus_pred, c_pred = y_pred[0], y_pred[1]\n",
    "        ## MSE loss\n",
    "        #cossin_loss = self.clambda[0]*self.cossin_loss(taus_actual, taus_pred)\n",
    "        ## PINN loss\n",
    "        #pinn_loss = self.clambda[1]*self.pinn_loss(taus_actual, taus_pred, c_pred)\n",
    "        ## speed los\n",
    "        #c_loss = self.clambda[2]*self.speed_loss(c_actual, c_pred)\n",
    "#\n",
    "        ##tf.print(pinn_loss, cossin_loss)\n",
    "#\n",
    "        ## Total loss\n",
    "        ##loss = cossin_loss + pinn_loss + c_loss\n",
    "        #return cossin_loss + c_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4cdeac-7624-4df4-8710-e1d93df43a1b",
   "metadata": {},
   "source": [
    "def mse(y_actual, y_predict):\n",
    "    err = tf.reduce_mean(tf.math.square(((y_actual - y_predict) + 180) % 360 -180))\n",
    "    return err\n",
    "def mae(y_actual, y_predict):\n",
    "    err = tf.reduce_mean(tf.math.abs(((y_actual - y_predict) + 180) % 360 -180))\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a9ad64-d63b-483f-baec-74cc91efefe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMSE(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='custom_mse', **kwargs):\n",
    "        super(CustomMSE, self).__init__(name=name, **kwargs)\n",
    "        # Initialize variables to track squared error and count\n",
    "        self.total_squared_error = self.add_weight(name='total_squared_error', initializer='zeros')\n",
    "        self.count = self.add_weight(name='count', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Compute squared error with cyclic distance correction\n",
    "        squared_error = tf.square(((y_true - y_pred) + 180) % 360 - 180)\n",
    "        \n",
    "        # Apply sample weight if available\n",
    "        if sample_weight is not None:\n",
    "            squared_error = tf.multiply(squared_error, sample_weight)\n",
    "\n",
    "        # Update the total error and count\n",
    "        self.total_squared_error.assign_add(tf.reduce_sum(squared_error))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        # Return the mean squared error\n",
    "        return self.total_squared_error / self.count\n",
    "\n",
    "    def reset_states(self):\n",
    "        # Reset the internal variables at the start of each epoch\n",
    "        self.total_squared_error.assign(0.0)\n",
    "        self.count.assign(0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c85e353-597e-497d-8550-cf61d860b2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMAE(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='custom_mse', **kwargs):\n",
    "        super(CustomMAE, self).__init__(name=name, **kwargs)\n",
    "        # Initialize variables to track squared error and count\n",
    "        self.total_squared_error = self.add_weight(name='total_squared_error', initializer='zeros')\n",
    "        self.count = self.add_weight(name='count', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Compute squared error with cyclic distance correction\n",
    "        squared_error = tf.abs(((y_true - y_pred) + 180) % 360 - 180)\n",
    "        \n",
    "        # Apply sample weight if available\n",
    "        if sample_weight is not None:\n",
    "            squared_error = tf.multiply(squared_error, sample_weight)\n",
    "\n",
    "        # Update the total error and count\n",
    "        self.total_squared_error.assign_add(tf.reduce_sum(squared_error))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        # Return the mean squared error\n",
    "        return self.total_squared_error / self.count\n",
    "\n",
    "    def reset_states(self):\n",
    "        # Reset the internal variables at the start of each epoch\n",
    "        self.total_squared_error.assign(0.0)\n",
    "        self.count.assign(0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ba9e580-03e7-4819-bdf9-1c05e6fcfb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_evaluation(list_channels, AA_geometry, \\\n",
    "                        inputs, speeds, labels, norm, clambda = [1.0,1.0,1.0],\n",
    "                        he_initializer=True, floormod=True, \n",
    "                        epochs = 50, plot = False, save_fig = False,verbose=False):\n",
    "    # trained models\n",
    "    models = []\n",
    "    # models losses history\n",
    "    losses = []\n",
    "    # evluation scores\n",
    "    evaluates = []\n",
    "    # reference channel at channel 0 for time delays\n",
    "    ref_geometry = tf.constant(AA_geometry[0][:2], dtype=float)\n",
    "\n",
    "    # train models for each selected channels in the list of channels\n",
    "    for channels in list_channels:\n",
    "        # Get the geometry of the channels only in x and y planes\n",
    "        array_geometry = tf.constant(AA_geometry[channels][:,:2], dtype=float)\n",
    "        # pack selected channels data into tensorflow dataset\n",
    "        dataset = DataSetPacker(inputs, speeds, labels, channels)\n",
    "        \n",
    "        # split dataset into train, validation, and test dataset\n",
    "        train_dataset, val_dataset, test_dataset = dataset.split(shuffle=False)\n",
    "        # Initialize the model\n",
    "        model = MiniPINN(np.shape(channels), array_geometry, he_initializer, floormod)\n",
    "        # Compile model\n",
    "        #model.compile(optimizer='adam', loss=MiniPINNLoss(array_geometry, ref_geometry, norm, clambda))\n",
    "        model.compile(optimizer='adam', loss=MiniPINNLoss())\n",
    "        l = model.fit(train_dataset.batch(32), epochs=epochs, validation_data = val_dataset.batch(32), verbose=verbose)\n",
    "        # Train model\n",
    "        # Get model score\n",
    "        eva = EvaluateModel(model, channels, test_dataset)\n",
    "        eva.evaluate(verbose=False)\n",
    "        #eva = EvaluateModel.evaluate(model, test_dataset, False)\n",
    "        if plot:\n",
    "            if save_fig:\n",
    "                eva.plot_training(l, save_dir = 'plot/history/')\n",
    "                eva.plot_evaluation(save_dir = 'plot/evaluation/')\n",
    "            else:\n",
    "                eva.plot_training(l)\n",
    "                eva.plot_evaluation()\n",
    "        models.append(model)\n",
    "        losses.append(l)\n",
    "        evaluates.append(eva)\n",
    "    return models, evaluates, l\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36db9172-574d-4391-8f46-4f9266034d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
